# ğŸŒ™ Lunaris Codex

**Lunaris Codex** is a custom Transformer Decoder architecture designed for code generation, written entirely in PyTorch with support for modern optimizations like **LoRA**, **FlashAttention**, and **ALiBi**. This repository includes the full source code for training and model definition â€” designed to be lightweight, scalable, and GPU-efficient.

> âš ï¸ **Note:** This release includes only the architecture and training system. Pretrained weights will be released in the future, trained on a massive-scale dataset using NVIDIA **H100 SXM** and **GH200** GPUs.

---

## âœ¨ Features

- âš™ï¸ **Decoder-only Transformer**, optimized for code and natural language
- ğŸš€ **LoRA support** (Low-Rank Adaptation) for efficient fine-tuning
- âš¡ **FlashAttention integration** for fast attention on modern GPUs
- ğŸ§  **ALiBi** positional bias for long-context support
- ğŸ“ ~183M parameters with default config (can be scaled easily)
- ğŸ§ª Simple, scalable **training pipeline** with AMP, torch.compile, and safetensors

---

## ğŸ§± Architecture Overview

Lunaris Codex follows a modern decoder-only Transformer structure with several enhancements:

- **10 transformer blocks** (configurable)
- **768 hidden size**, **12 attention heads**
- **SwigLU activation** in the feedforward blocks
- **ALiBi** is used instead of absolute positional embeddings
- **LoRA** is applied to all projection layers (`qkv`, `output`, and feedforward)
- **FlashAttention** replaces traditional attention computation
- **LayerScale** stabilizes residual connections
- **Final tied embedding head** for efficiency and weight sharing

> All design decisions are optimized for training speed, memory efficiency, and compatibility with large-scale tokenizers like StarCoder.

---

## ğŸ§¢ Training

The repository includes a full training loop (`train.py`) compatible with custom datasets. The model uses:

- `torch.cuda.amp` (mixed precision)
- `AdamW` optimizer with fused operations
- Checkpointing using `.safetensors`
- Real-time metrics: **loss**, **perplexity**, and **top-1 accuracy**

The model is designed for training on **multi-GPU** setups with full support for TF32/BF16 performance on **NVIDIA A100**, **H100**, and **GH200**.

---

## ğŸ§  What's included

| Component                  | Status         |
|---------------------------|----------------|
| Model architecture         | âœ… Released    |
| LoRA integration           | âœ… Released    |
| FlashAttention support     | âœ… Released    |
| ALiBi + LayerScale         | âœ… Released    |
| Training pipeline          | âœ… Released    |
| Dataset preprocessing      | âŒ Private     |
| Inference system (CLI/UI)  | âŒ Private     |
| Pretrained weights         | ğŸ”œ Coming soon |

---

## âš ï¸ Roadmap

- ğŸ§¬ Release **pretrained checkpoints** trained on ~1B+ tokens of high-quality code
- ğŸ“¦ Public **inference UI / API**
- ğŸ“– Hugging Face model card & demo
- ğŸŒ Documentation for integrating with LangChain, Discord bots, or VSCode plugins

---

## ğŸ“¦ Installation

```bash
pip install -r requirements.txt
```

Dependencies include:
- `torch`
- `transformers`
- `safetensors`
- `numpy`
- `datasets` (for custom extensions)

---

## ğŸ“ License

This project is licensed under the **MIT License**.  
Copyright (c) 2025 **Francisco Antonio**

See [`LICENSE`](LICENSE) for more details.

---

## ğŸŒŸ Credits

Developed by [Francisco Antonio](https://github.com/MeryylleA)  
Feel free to star â­ the project and share feedback or contributions!

> For collaborations, dataset requests, or research interest: contact via GitHub or [@Meryylle](https://x.com/a93918)

---

## ğŸŒŒ Why "Lunaris"?

> *"Because great ideas are born in silence â€” and shine like the moon."* ğŸŒ™
