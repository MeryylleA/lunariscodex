# Lunaris Codex

![Lunaris Codex](lunaris-codex.jpg)

Welcome to the repository of **Lunaris Codex**, a 100% Brazilian Natural Language Processing (NLP) project focused on programming. Built from scratch, Lunaris Codex is a Transformer-based model designed to generate and interpret code in multiple languages, such as Python, JavaScript, Java, and Rust, with a special emphasis on Python.

## Summary

- **Objective**: Develop an efficient AI tool for programmers to assist with code generation, interpretation, and technical problem-solving.
- **Technology**: Transformer architecture optimized with Alibi (replacing UOP for better efficiency), trained on 617 million tokens, with plans to expand to 2-4 billion tokens.
- **Datasets**: The Stack V2, FineWeb, Hugging Face Datasets, and other technical code-focused datasets, prioritizing Python.
- **Startup**: Developed by Moon Cloud Services, a company dedicated to innovation in AI and neural networks.

## Technologies Used

![PyTorch](https://img.shields.io/badge/PyTorch-%23007ACC.svg?style=flat&logo=PyTorch&logoColor=white)
![Transformers](https://img.shields.io/badge/Transformers-%23FFD21E.svg?style=flat&logo=HuggingFace&logoColor=black)
![Hugging%20Face%20Datasets](https://img.shields.io/badge/HF%20Datasets-%23FFD21E.svg?style=flat&logo=HuggingFace&logoColor=black)
![Python](https://img.shields.io/badge/Python-%234B8BBE.svg?style=flat&logo=Python&logoColor=white)
![NVIDIA](https://img.shields.io/badge/NVIDIA-%2350C878.svg?style=flat&logo=NVIDIA&logoColor=white)
![OVHcloud](https://img.shields.io/badge/OVHcloud-%23123F7D.svg?style=flat&logo=OVH&logoColor=white)

- **Framework**: Built using PyTorch for model development and training.
- **Libraries**: Leveraging Hugging Face Transformers for architecture and tokenization, and Hugging Face Datasets for high-quality data curation.
- **Programming**: Primarily coded in Python, with support for generating code in multiple languages.
- **Hardware**: Trained on 4 NVIDIA H100 GPUs for high-performance computing.
- **Infrastructure**: Hosted on OVHcloud servers with $10,000 in credits for scalable processing.

## Project Details

- **Model Size**: Between 160 million and 260 million parameters, balancing efficiency and capability.
- **Training Goals**: Expanding the dataset to 2-4 billion tokens for a full training run, enhancing accuracy and versatility.
- **Features**: Code generation, interpretation, debugging assistance, and optimization of programming tasks.
- **Open Source**: The source code will be released on GitHub under an open-source license, with restrictions to prevent unauthorized modifications.

## Status

Lunaris Codex is under active development and will be available soon as an open-source project on this GitHub repository. Stay tuned for updates!

## Contact

Want to learn more? Follow our progress:
- [LinkedIn](https://www.linkedin.com/in/francisco-antonio-0434aa284/)
- [Email](mailto:business@mooncloudservices.tech)
- [Discord](https://discord.gg/FmnSVTeSYF)

Â© 2025 Moon Cloud Services
