name: Reusable C++ Build and Test

on:
  workflow_call:
    inputs:
      test_matrix_json:
        description: 'JSON string of the OS matrix for tests'
        required: true
        type: string
      python_version_for_fixtures:
        description: 'Python version to use for generating test fixtures'
        required: false
        type: string
        default: '3.11'
    outputs:
      cpp_test_result:
        description: "Result of the C++ tests"
        value: ${{ jobs.cpp_utilities.result }}
    secrets: # Se prepare_data.py precisar de HF_TOKEN para datasets/tokenizers
      HF_TOKEN:
        required: false


env:
  CXXFLAGS_MODE: RELEASE
  PYTHONUTF8: "1"

jobs:
  cpp_utilities:
    name: C++ Build & Test (${{ matrix.os.display }})
    runs-on: ${{ matrix.os.runner }}
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(inputs.test_matrix_json) }}
    defaults:
      run:
        shell: bash
    outputs:
      text_cleaner_exec_path: ${{ steps.executables.outputs.text_cleaner_exec }}
      data_analyzer_exec_path: ${{ steps.executables.outputs.data_analyzer_exec }}
      bpe_processor_exec_path: ${{ steps.executables.outputs.bpe_processor_exec }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Install Build Dependencies (Ubuntu)
        run: |
          sudo apt-get update -y
          sudo apt-get install -y nlohmann-json3-dev build-essential jq python3-pip
          g++ --version
          python3 --version || echo "Python3 not found, fixture generation might fail"
          pip3 --version || echo "pip3 not found, fixture generation might fail"

      - name: Cache C++ Executables
        id: cache_executables
        uses: actions/cache@v4
        with:
          path: |
            ${{ github.workspace }}/text_cleaner/lunaris_text_cleaner
            ${{ github.workspace }}/data_analyzer/lunaris_data_analyzer
            ${{ github.workspace }}/bpe_trainer/bpe_processor
          key: ${{ matrix.os.runner }}-cpp-exec-v1-${{ hashFiles('Makefile', 'text_cleaner/**/*.cpp', 'text_cleaner/**/*.hpp', 'data_analyzer/**/*.cpp', 'data_analyzer/**/*.hpp', 'bpe_trainer/**/*.cpp', 'bpe_trainer/**/*.hpp') }}
          restore-keys: |
            ${{ matrix.os.runner }}-cpp-exec-v1-

      - name: Clean Previous Build Artifacts (if no cache hit)
        if: steps.cache_executables.outputs.cache-hit != 'true'
        run: make clean || echo "Clean failed or no artifacts to clean."

      - name: Build C++ Utilities (if no cache hit)
        if: steps.cache_executables.outputs.cache-hit != 'true'
        run: make all CXXFLAGS_MODE=${{ env.CXXFLAGS_MODE }}
      
      - name: List files after potential build/cache restore
        run: |
          ls -lA ${{ github.workspace }}/
          ls -lA ${{ github.workspace }}/text_cleaner/ || true
          ls -lA ${{ github.workspace }}/data_analyzer/ || true
          ls -lA ${{ github.workspace }}/bpe_trainer/ || true

      - name: Set and Verify Executable Paths
        id: executables
        run: |
          TC_EXEC="${{ github.workspace }}/text_cleaner/lunaris_text_cleaner"
          DA_EXEC="${{ github.workspace }}/data_analyzer/lunaris_data_analyzer"
          BPE_EXEC="${{ github.workspace }}/bpe_trainer/bpe_processor"
          echo "text_cleaner_exec=$TC_EXEC" >> $GITHUB_OUTPUT
          echo "data_analyzer_exec=$DA_EXEC" >> $GITHUB_OUTPUT
          echo "bpe_processor_exec=$BPE_EXEC" >> $GITHUB_OUTPUT
          all_execs_found=true
          for exec_file in "$TC_EXEC" "$DA_EXEC" "$BPE_EXEC"; do
            if [ ! -f "$exec_file" ] || [ ! -x "$exec_file" ]; then
              echo "ERROR: Executable problem: $exec_file"; all_execs_found=false; fi
          done
          if [[ "$all_execs_found" != "true" ]]; then exit 1; fi
          echo "All C++ executables verified."

      - name: Run Text Cleaner Tests
        run: |
          TEXT_CLEANER_EXEC="${{ steps.executables.outputs.text_cleaner_exec }}"
          mkdir -p ./temp_text_cleaner_input ./temp_text_cleaner_output
          cat <<EOF_TC_INPUT > ./temp_text_cleaner_input/sample.txt
          <!DOCTYPE html>
          <html> <head><title>Test</title></head> <body>
          <!-- This is a comment -->
          <p>Hello   World!  </p>
          Another line.
          URL: http://example.com and email: test@example.com
          Duplicate Line
          Duplicate Line
          </body> </html>
          EOF_TC_INPUT

          "$TEXT_CLEANER_EXEC" \
            --input ./temp_text_cleaner_input/sample.txt \
            --output ./temp_text_cleaner_output/cleaned.txt \
            --remove-html --normalize-whitespace --remove-empty-lines \
            --to-lowercase \
            --process-urls --url-placeholder "[url]" \
            --process-emails --email-placeholder "[email]" \
            --remove-exact-duplicates

          cat <<EXPECTED_TC_EOF > expected_output.txt
          test
          hello world!
          another line.
          url: [url] and email: [email]
          duplicate line
          EXPECTED_TC_EOF

          if diff -u expected_output.txt ./temp_text_cleaner_output/cleaned.txt; then
            echo "✅ Text Cleaner test passed on ${{ matrix.os.display }}"
          else
            echo "❌ Text Cleaner test failed on ${{ matrix.os.display }}"
            echo "--- Expected ---"; cat expected_output.txt; echo "--- Actual ---"; cat ./temp_text_cleaner_output/cleaned.txt
            exit 1
          fi

      - name: Run BPE Processor Tests
        run: |
          BPE_EXEC="${{ steps.executables.outputs.bpe_processor_exec }}"
          mkdir -p ./temp_bpe/corpus ./temp_bpe/model_output
          echo -e "hello world this is a test a test\nanother line for another test of the bpe\nhello world again" > ./temp_bpe/corpus/corpus.txt
          "$BPE_EXEC" --action train --corpus ./temp_bpe/corpus/corpus.txt --vocab-size 270 --output ./temp_bpe/model_output/bpe_model/ --mode byte --verbose
          for file in "./temp_bpe/model_output/bpe_model/bpe_model_lunaris.json" "./temp_bpe/model_output/bpe_model/merges_lunaris.txt" "./temp_bpe/model_output/bpe_model/vocabulary_lunaris.txt"; do
            if [ ! -f "$file" ]; then echo "ERROR: Required BPE output file not found: $file"; ls -R ./temp_bpe/model_output/; exit 1; fi
          done
          TOKEN_OUTPUT=$("$BPE_EXEC" --action tokenize --model_path "./temp_bpe/model_output/bpe_model/" --input_text "hello test world" --verbose)
          if [ $? -ne 0 ] || [ -z "$TOKEN_OUTPUT" ]; then echo "ERROR: BPE tokenization failed. Output: $TOKEN_OUTPUT"; exit 1; fi
          echo "✅ BPE Processor test passed on ${{ matrix.os.display }}"

      - name: Setup Python for Fixture Generation
        uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python_version_for_fixtures }}

      - name: Install Python Dependencies for Fixtures
        run: |
          python -m pip install --upgrade pip
          pip install transformers huggingface_hub numpy datasets

      - name: Prepare Data Analyzer Test Fixtures
        id: da_fixtures
        env:
           HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          TEMP_FIXTURE_DIR="./temp_da_tests"
          mkdir -p "$TEMP_FIXTURE_DIR/processed_data" "$TEMP_FIXTURE_DIR/tokenizer_starcoder" "$TEMP_FIXTURE_DIR/sample_code_input"
          TOKENIZER_OUTPUT_DIR="$TEMP_FIXTURE_DIR/tokenizer_starcoder"
          MEMMAP_OUTPUT_PATH="$TEMP_FIXTURE_DIR/processed_data/starcoder_sample.memmap"
          STARCODER_TOKENIZER_JSON="$TOKENIZER_OUTPUT_DIR/tokenizer.json"
          PREPARE_DATA_SCRIPT="${{ github.workspace }}/prepare_data.py"

          echo "Downloading starcoder tokenizer..."
          python -c "from transformers import AutoTokenizer; tokenizer = AutoTokenizer.from_pretrained('bigcode/starcoder', trust_remote_code=True); tokenizer.save_pretrained('$TOKENIZER_OUTPUT_DIR')"
          if [ ! -f "$STARCODER_TOKENIZER_JSON" ]; then echo "ERROR: Starcoder tokenizer.json not found!"; ls -R "$TOKENIZER_OUTPUT_DIR"; exit 1; fi
          echo "Starcoder tokenizer downloaded."

          cat <<EOF_CODE > "$TEMP_FIXTURE_DIR/sample_code_input/sample_code.py"
          def hello_world():
              print("Hello, world from Lunaris Codex!")
          class MyClass:
              def __init__(self, name):
                  self.name = name
          EOF_CODE
          echo "Sample code for prepare_data.py created."

          if [ ! -f "$PREPARE_DATA_SCRIPT" ]; then echo "ERROR: prepare_data.py not found at $PREPARE_DATA_SCRIPT!"; exit 1; fi
          echo "Running prepare_data.py to create memmap..."
          python "$PREPARE_DATA_SCRIPT" \
            --data_source_type text_file_lines \
            --dataset_name_or_path "$TEMP_FIXTURE_DIR/sample_code_input/sample_code.py" \
            --tokenizer_name_or_path "$TOKENIZER_OUTPUT_DIR" \
            --output_path "$MEMMAP_OUTPUT_PATH" \
            --max_length 32 \
            --max_examples 2 \
            --add_special_tokens \
            --overwrite_output
          if [ ! -f "$MEMMAP_OUTPUT_PATH" ]; then echo "ERROR: Sample memmap not created by prepare_data.py!"; exit 1; fi
          echo "Sample memmap created."

          echo "memmap_file=$MEMMAP_OUTPUT_PATH" >> $GITHUB_OUTPUT
          echo "hf_json_vocab_file=$STARCODER_TOKENIZER_JSON" >> $GITHUB_OUTPUT
          echo "Data Analyzer test fixtures generated."

      - name: Run Data Analyzer Tests - Text Output with Starcoder Decoding
        run: |
          DA_EXEC="${{ steps.executables.outputs.data_analyzer_exec }}"
          MEMMAP_FILE="${{ steps.da_fixtures.outputs.memmap_file }}"
          HF_JSON_VOCAB_FILE="${{ steps.da_fixtures.outputs.hf_json_vocab_file }}"
          if [ ! -f "$DA_EXEC" ] || [ ! -f "$MEMMAP_FILE" ] || [ ! -f "$HF_JSON_VOCAB_FILE" ]; then echo "ERROR: Missing files for DA test"; exit 1; fi

          OUTPUT=$("$DA_EXEC" --file "$MEMMAP_FILE" --num_sequences 2 --max_length 32 --pad_id 0 \
            --print_seq 2 --top_n_tokens 5 \
            --vocab_file "$HF_JSON_VOCAB_FILE" --tokenizer_type hf_json)
          echo "--- Data Analyzer Output (Starcoder Decoded) ---"; echo "$OUTPUT"; echo "------------------------------------------------"
          echo "$OUTPUT" | grep -q "Sequence 0:" || (echo "ERROR: Starcoder Decoded Seq 0 not found"; exit 1)
          echo "$OUTPUT" | grep -q "padding_token_percentage" || (echo "ERROR: Padding percentage not found"; exit 1)
          echo "✅ Data Analyzer text output with Starcoder decoding test (basic check) passed."

      - name: Run Data Analyzer Tests - JSON Output with Starcoder Decoding
        run: |
          DA_EXEC="${{ steps.executables.outputs.data_analyzer_exec }}"
          MEMMAP_FILE="${{ steps.da_fixtures.outputs.memmap_file }}"
          HF_JSON_VOCAB_FILE="${{ steps.da_fixtures.outputs.hf_json_vocab_file }}"
          if [ ! -f "$DA_EXEC" ] || [ ! -f "$MEMMAP_FILE" ] || [ ! -f "$HF_JSON_VOCAB_FILE" ]; then echo "ERROR: Missing files for DA JSON test"; exit 1; fi

          JSON_OUTPUT=$("$DA_EXEC" --file "$MEMMAP_FILE" --num_sequences 2 --max_length 32 --pad_id 0 \
            --top_n_tokens 5 --output_format json \
            --vocab_file "$HF_JSON_VOCAB_FILE" --tokenizer_type hf_json)
          echo "--- Data Analyzer JSON Output (Starcoder Decoded) ---"; echo "$JSON_OUTPUT" | jq .; echo "---------------------------------------------------"
          TOTAL_TOKENS=$(echo "$JSON_OUTPUT" | jq '.total_tokens_in_file')
          if [[ -z "$TOTAL_TOKENS" || "$TOTAL_TOKENS" == "null" ]]; then echo "ERROR: JSON total_tokens is null/empty"; exit 1; fi
          echo "$JSON_OUTPUT" | jq -e '.top_n_common_tokens[0].token_string' > /dev/null || (echo "ERROR: JSON top token string not found/invalid"; exit 1)
          echo "✅ Data Analyzer JSON output with Starcoder decoding test (basic check) passed."
      
      - name: Run Data Analyzer Tests - Argument Validation
        run: |
          DA_EXEC="${{ steps.executables.outputs.data_analyzer_exec }}"
          MEMMAP_FILE="${{ steps.da_fixtures.outputs.memmap_file }}"
          HF_JSON_VOCAB_FILE="${{ steps.da_fixtures.outputs.hf_json_vocab_file }}"
          if [ ! -f "$DA_EXEC" ] || [ ! -f "$MEMMAP_FILE" ] || [ ! -f "$HF_JSON_VOCAB_FILE" ]; then echo "ERROR: Missing files for DA arg validation test"; exit 1; fi
          
          set +e
          "$DA_EXEC" --file "$MEMMAP_FILE" --num_sequences 2 --max_length 32 --vocab_file "$HF_JSON_VOCAB_FILE" >/dev/null 2>&1
          EXIT_CODE=$?
          set -e
          if [[ $EXIT_CODE -eq 0 ]]; then echo "ERROR: DA should have failed for missing tokenizer_type"; exit 1; fi
          echo "✅ Data Analyzer correctly failed/warned for missing tokenizer_type (Exit code: $EXIT_CODE)."

      - name: Cleanup All Test Files
        if: always()
        run: rm -rf ./temp_text_cleaner_input ./temp_text_cleaner_output ./expected_output.txt ./temp_bpe ./temp_da_tests
