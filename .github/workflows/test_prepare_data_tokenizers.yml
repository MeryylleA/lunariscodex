name: Test prepare_data.py with Multiple Tokenizers

on:
  push:
    branches: [ main ] # Ou configure para rodar apenas em PRs para main, ou manualmente
    paths: # Opcional: rodar apenas se prepare_data.py ou este workflow mudar
      - 'prepare_data.py'
      - '.github/workflows/test_prepare_data_tokenizers.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'prepare_data.py'
      - '.github/workflows/test_prepare_data_tokenizers.yml'
  workflow_dispatch: # Permite acionamento manual

jobs:
  test-prepare-data:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false # Continua outros jobs da matriz mesmo se um falhar
      matrix:
        tokenizer:
          - gpt2
          - bert-base-uncased
          - roberta-base
          - bigcode/starcoder # Exemplo de outro tipo
          # Adicione mais tokenizadores aqui conforme necessário
        # python-version: [ '3.10', '3.11' ] # Exemplo se quisesse testar com várias versões de Python

    name: Test prepare_data with ${{ matrix.tokenizer }} # Nome dinâmico para o job

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11 # Ou use ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: '3.11' # Ou ${{ matrix.python-version }}

      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-py${{ matrix.python-version || '3.11' }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-py${{ matrix.python-version || '3.11' }}-

      - name: Install Python dependencies
        run: |
          python -m venv .venv-tokenizer-test
          source .venv-tokenizer-test/bin/activate
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create dummy dataset for prepare_data.py
        run: |
          mkdir -p ./temp_prepare_data_multi_tokenizer_input
          # Usar um texto simples que funcione bem com a maioria dos tokenizadores
          # Pode ser interessante ter alguns textos com casos especiais também
          cat <<EOF > ./temp_prepare_data_multi_tokenizer_input/sample.txt
          This is the first sentence. It has some punctuation!
          Here is another sentence for tokenization. Lunaris Codex is cool.
          "Quoted text" with numbers 123 and symbols like #
          EOF
          mkdir -p ./temp_prepare_data_multi_tokenizer_output

      - name: Run prepare_data.py with tokenizer ${{ matrix.tokenizer }}
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }} 
        run: |
          source .venv-tokenizer-test/bin/activate
          echo "--- Preparing Data with Tokenizer: ${{ matrix.tokenizer }} ---"
          
          # Nome de output único para cada tokenizador
          OUTPUT_DIR_BASE="./temp_prepare_data_multi_tokenizer_output"
          TOKENIZER_NAME_SLUG=$(echo "${{ matrix.tokenizer }}" | tr '/' '_') # Converte 'bigcode/starcoder' para 'bigcode_starcoder'
          OUTPUT_MEMMAP_FILE="$OUTPUT_DIR_BASE/data_${TOKENIZER_NAME_SLUG}.memmap"
          
          # Limpar output anterior se existir (para execuções locais ou re-runs)
          rm -f "$OUTPUT_MEMMAP_FILE" 

          python prepare_data.py \
            --data_source_type text_file_lines \
            --dataset_name_or_path "./temp_prepare_data_multi_tokenizer_input/sample.txt" \
            --tokenizer_name_or_path ${{ matrix.tokenizer }} \
            --max_length 128 \
            --output_path "$OUTPUT_MEMMAP_FILE" \
            --hf_formatting_template "{text}" \
            --add_special_tokens # Geralmente uma boa ideia para testar
            # --max_examples 3 # Se o arquivo de input tiver muitas linhas

          # Verificação básica: o arquivo memmap foi criado?
          if [ ! -f "$OUTPUT_MEMMAP_FILE" ]; then
            echo "ERROR: Memmap file '$OUTPUT_MEMMAP_FILE' not created for tokenizer ${{ matrix.tokenizer }}"
            exit 1
          else
            echo "Memmap file '$OUTPUT_MEMMAP_FILE' created successfully for tokenizer ${{ matrix.tokenizer }}."
            # Opcional: Uma verificação de tamanho de arquivo (não vazio)
            if [ ! -s "$OUTPUT_MEMMAP_FILE" ]; then
                echo "ERROR: Memmap file '$OUTPUT_MEMMAP_FILE' is empty for tokenizer ${{ matrix.tokenizer }}"
                exit 1
            fi
            echo "Memmap file for ${{ matrix.tokenizer }} is not empty."
          fi
      
      # Opcional: Passo de Validação com lunaris_data_analyzer
      # Para isso, você precisaria compilar o lunaris_data_analyzer aqui também.
      # Se você decidir fazer isso, adicione os steps de cache e build para ele.
      # - name: Build C++ Data Analyzer (if needed for validation)
      #   run: |
      #     cd data_analyzer
      #     g++ lunaris_data_analyzer.cpp -o lda_ci_executable -std=c++17 -O2 || { echo "Data Analyzer compilation failed"; exit 1; }
      #     cd ..

      # - name: Validate memmap file with lunaris_data_analyzer for ${{ matrix.tokenizer }}
      #   run: |
      #     source .venv-tokenizer-test/bin/activate
      #     OUTPUT_DIR_BASE="./temp_prepare_data_multi_tokenizer_output"
      #     TOKENIZER_NAME_SLUG=$(echo "${{ matrix.tokenizer }}" | tr '/' '_')
      #     MEMMAP_FILE_TO_VALIDATE="$OUTPUT_DIR_BASE/data_${TOKENIZER_NAME_SLUG}.memmap"
      #     NUM_SEQUENCES_EXPECTED=$(wc -l < ./temp_prepare_data_multi_tokenizer_input/sample.txt) # Exemplo de como obter
            
      #     # Verifique se o analisador foi compilado
      #     if [ ! -f ./data_analyzer/lda_ci_executable ]; then
      #        echo "Skipping data_analyzer validation as executable not found."
      #     else
      #        echo "Validating $MEMMAP_FILE_TO_VALIDATE..."
      #        ./data_analyzer/lda_ci_executable \
      #          --file "$MEMMAP_FILE_TO_VALIDATE" \
      #          --num_sequences "$NUM_SEQUENCES_EXPECTED" \ # Você precisa saber quantas sequências esperar
      #          --max_length 128 \
      #          --dtype int32 \
      #          --print_seq 0 || { echo "lunaris_data_analyzer validation failed for ${{ matrix.tokenizer }}"; exit 1; }
      #        echo "lunaris_data_analyzer validation passed for ${{ matrix.tokenizer }}."
      #     fi

      - name: Clean up temporary files
        if: always()
        run: |
          rm -rf .venv-tokenizer-test \
                 ./temp_prepare_data_multi_tokenizer_input \
                 ./temp_prepare_data_multi_tokenizer_output
          # Se você compilar o data_analyzer, limpe-o também:
          # rm -f ./data_analyzer/lda_ci_executable
          echo "Cleaned up tokenizer test temporary files."
