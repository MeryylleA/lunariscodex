# .github/workflows/ci.yml
name: Lunaris Codex CI

on:
  push:
    branches: [ main ]
    paths: # Only run if changes occur in these paths
      - 'model.py'
      - 'prepare_data.py'
      - 'train.py'
      - 'inference.py'
      - 'text_cleaner/**'
      - 'data_analyzer/**'
      - 'bpe_trainer/**' # Added for BPE trainer
      - 'tests/**'
      - 'Makefile'
      - 'requirements.txt'
      - '.github/workflows/ci.yml' # Changes to the workflow itself
  pull_request:
    branches: [ main ]
    paths: # Same logic for PRs
      - 'model.py'
      - 'prepare_data.py'
      - 'train.py'
      - 'inference.py'
      - 'text_cleaner/**'
      - 'data_analyzer/**'
      - 'bpe_trainer/**' # Added
      - 'tests/**'
      - 'Makefile'
      - 'requirements.txt'
      - '.github/workflows/ci.yml'
  workflow_dispatch: # Allows manual triggering

jobs:
  #---------------------------------------------------------------------------
  # JOB 1: Build and Test C++ Utilities
  #---------------------------------------------------------------------------
  build_and_test_cpp:
    name: Build & Test C++ Utilities
    runs-on: ubuntu-latest
    outputs: # To potentially pass executable names to other jobs, if needed
      text_cleaner_exec: ${{ steps.set_exec_paths.outputs.text_cleaner_exec }}
      data_analyzer_exec: ${{ steps.set_exec_paths.outputs.data_analyzer_exec }}
      bpe_trainer_exec: ${{ steps.set_exec_paths.outputs.bpe_trainer_exec }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install C++ dependencies (nlohmann-json)
        run: |
          sudo apt-get update -y
          sudo apt-get install -y nlohmann-json3-dev # Common package name for nlohmann json v3

      - name: Initial C++ Artifact Cleanup (Best Effort)
        run: |
          make clean || echo "Pre-run 'make clean' info: No Makefile/artifacts or clean failed, proceeding."

      - name: Cache C++ build artifacts (executables)
        id: cache-cpp-executables
        uses: actions/cache@v4
        with:
          path: | # Paths to executables generated by the simplified Makefile
            text_cleaner/lunaris_text_cleaner
            data_analyzer/lunaris_data_analyzer
            bpe_trainer/bpe_trainer
          key: ${{ runner.os }}-cpp-exec-${{ hashFiles('Makefile', 'text_cleaner/**/*.cpp', 'data_analyzer/**/*.cpp', 'bpe_trainer/**/*.cpp') }}
          restore-keys: |
            ${{ runner.os }}-cpp-exec-

      - name: Build C++ Utilities (if not cached)
        if: steps.cache-cpp-executables.outputs.cache-hit != 'true'
        run: |
          make all CXXFLAGS_MODE=RELEASE # Force Release build for CI
          echo "C++ utilities built."

      - name: Set C++ executable paths
        id: set_exec_paths # This step will always run to set paths, whether built or cached
        run: |
          echo "text_cleaner_exec=text_cleaner/lunaris_text_cleaner" >> $GITHUB_OUTPUT
          echo "data_analyzer_exec=data_analyzer/lunaris_data_analyzer" >> $GITHUB_OUTPUT
          echo "bpe_trainer_exec=bpe_trainer/bpe_trainer" >> $GITHUB_OUTPUT
          if [[ "${{ steps.cache-cpp-executables.outputs.cache-hit }}" == "true" ]]; then
            echo "C++ executables were restored from cache."
          fi

      - name: Create dummy text file for Text Cleaner CI
        run: |
          mkdir -p ./temp_text_cleaner_ci_input
          cat <<EOF > ./temp_text_cleaner_ci_input/sample_for_cleaner.txt
          <!DOCTYPE html>
          <html> <head><title>Test</title></head> <body>
          <!-- This is a comment -->
          <p>Hello   World!  </p>
          <script>alert("script content");</script>
          Another line.
          URL: http://example.com and email: test@example.com
          Duplicate Line
          Duplicate Line
          </body> </html>
          EOF
          mkdir -p ./temp_text_cleaner_ci_output

      - name: Run and Test C++ Text Cleaner
        run: |
          TEXT_CLEANER_EXEC="./${{ steps.set_exec_paths.outputs.text_cleaner_exec }}"
          if [ ! -f "$TEXT_CLEANER_EXEC" ]; then echo "ERROR: Text Cleaner executable '$TEXT_CLEANER_EXEC' not found!"; exit 1; fi
          
          "$TEXT_CLEANER_EXEC" \
            --input ./temp_text_cleaner_ci_input/sample_for_cleaner.txt \
            --output ./temp_text_cleaner_ci_output/cleaned_sample.txt \
            --remove-html --normalize-whitespace --remove-empty-lines \
            --to-lowercase \
            --process-urls --url-placeholder "[url]" \
            --process-emails --email-placeholder "[email]" \
            --remove-exact-duplicates || { echo "Text Cleaner execution failed"; exit 1; }
          
          cat <<-EXPECTED_EOF > expected_output_tc.txt
          test
          hello world!
          another line.
          url: [url] and email: [email]
          duplicate line
          EXPECTED_EOF
          
          echo "--- Actual Cleaned Output (from file) ---"
          cat ./temp_text_cleaner_ci_output/cleaned_sample.txt
          echo "--- Expected Output ---"
          cat expected_output_tc.txt

          if diff -a -u expected_output_tc.txt ./temp_text_cleaner_ci_output/cleaned_sample.txt; then
            echo "Text Cleaner output matches expected content."
          else
            echo "ERROR: Text Cleaner output does NOT match expected content. Diff shown above."
            exit 1
          fi

      - name: Create dummy data for BPE Trainer CI
        run: |
          mkdir -p ./temp_bpe_corpus
          echo "this is a test corpus for bpe" > ./temp_bpe_corpus/corpus1.txt
          echo "another line for the bpe test" >> ./temp_bpe_corpus/corpus1.txt
          echo "lunaris codex bpe trainer example" > ./temp_bpe_corpus/corpus2.txt
          mkdir -p ./temp_bpe_output

      - name: Run and Test C++ BPE Trainer
        run: |
          BPE_TRAINER_EXEC="./${{ steps.set_exec_paths.outputs.bpe_trainer_exec }}"
          if [ ! -f "$BPE_TRAINER_EXEC" ]; then echo "ERROR: BPE Trainer executable '$BPE_TRAINER_EXEC' not found!"; exit 1; fi
          
          "$BPE_TRAINER_EXEC" \
            --corpus_dir ./temp_bpe_corpus \
            --vocab_size 280 \
            --min_frequency 1 \
            --output_basename ./temp_bpe_output/ci_bpe \
            --mode byte || { echo "BPE Trainer execution failed"; exit 1; }
            
          if [ ! -f "./temp_bpe_output/ci_bpe_vocab.json" ] || [ ! -f "./temp_bpe_output/ci_bpe_merges.txt" ]; then
            echo "ERROR: BPE Trainer output files not found!"
            ls -R ./temp_bpe_output
            exit 1
          fi
          echo "BPE Trainer ran successfully and created output files."
          # Add content checks if necessary

      - name: Final C++ Cleanup for this Job
        if: always()
        run: |
          make clean || echo "Info: 'make clean' failed or no Makefile. C++ artifacts might not have been cleaned by make."
          rm -rf ./temp_text_cleaner_ci_input ./temp_text_cleaner_ci_output expected_output_tc.txt \
                 ./temp_bpe_corpus ./temp_bpe_output
          echo "C++ job cleanup complete."

  #---------------------------------------------------------------------------
  # JOB 2: Test Python Suite (prepare_data, train, inference, unit tests)
  #---------------------------------------------------------------------------
  test_python_suite:
    name: Test Python Suite
    runs-on: ubuntu-latest
    needs: [] # Can run in parallel with C++ if no direct dependency on C++ executables from the C++ job for its tests

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache Python dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt # Ensure pytest, pytest-cov, rich, etc., are here

      - name: Run Pytest for model.py (Unit Tests)
        run: |
          # Assuming your tests are in tests/test_model.py or similar
          # And you want coverage for model.py
          # Adjust the command if your test discovery or coverage target is different.
          # Example: pytest tests/ --cov=model --cov-report=xml --cov-report=term-missing
          # If tests for model are tagged or in a specific file:
          pytest tests/ -k "test_model" --cov=model --cov-report=xml --cov-report=term-missing
          # Or if all tests in 'tests/' folder should run and 'model.py' is the primary target for coverage:
          # pytest tests/ --cov=model --cov-report=xml --cov-report=term-missing

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.CODECOV_TOKEN }} # Add this secret in GitHub repository settings
          files: ./coverage.xml # Generated by pytest --cov-report=xml
          flags: model-tests # A flag for Codecov UI
          # fail_ci_if_error: true # Uncomment if you want the CI to fail if Codecov upload fails

      - name: Create dummy datasets for Python script CI
        run: |
          mkdir -p ./temp_data_ci_train
          echo "def train_function_one(): return 'train1'" > ./temp_data_ci_train/train_sample_1.py
          echo "class TrainSampleClass:\n  value = 'train2'" > ./temp_data_ci_train/train_sample_2.py
          mkdir -p ./temp_data_ci_val
          echo "def validation_function(): return 'validation_data_here'" > ./temp_data_ci_val/val_sample_1.py
          echo "# Another validation line" > ./temp_data_ci_val/val_sample_2.py

      - name: Run prepare_data.py for CI datasets
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }} # If prepare_data.py uses Hugging Face private models/datasets
        run: |
          python prepare_data.py \
            --data_source_type text_file_lines \
            --dataset_name_or_path "./temp_data_ci_train/*.py" \
            --tokenizer_name_or_path gpt2 \
            --output_path ./processed_data_ci/ci_train_data.memmap \
            --max_length 32 --max_examples 2 --overwrite_output

          python prepare_data.py \
            --data_source_type text_file_lines \
            --dataset_name_or_path "./temp_data_ci_val/*.py" \
            --tokenizer_name_or_path gpt2 \
            --output_path ./processed_data_ci/ci_val_data.memmap \
            --max_length 32 --max_examples 2 --overwrite_output

      - name: Run train.py (toy model with validation)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }} # If train.py uses Hugging Face
        run: |
          python train.py \
            --memmap_file_train ./processed_data_ci/ci_train_data.memmap \
            --num_sequences_train 2 \
            --memmap_file_val ./processed_data_ci/ci_val_data.memmap \
            --num_sequences_val 2 \
            --tokenizer_name_or_path gpt2 \
            --dataset_max_length 32 --dataset_dtype int32 \
            --model_max_seq_len 32 --d_model 32 --n_layers 1 --n_heads 1 \
            --batch_size 1 --num_epochs 1 --device cpu \
            --checkpoint_dir ./checkpoints_ci \
            --log_interval 1 --save_strategy epoch \
            --lora_rank 0 --seed 42
            
      - name: Check for checkpoint files
        id: check_ckpts
        run: |
          echo "Listing contents of checkpoint directory: ./checkpoints_ci"
          ls -R ./checkpoints_ci
          # Adjust pattern if your epoch/step naming convention is different or not fixed
          MAIN_CKPT_FILE_PATTERN="./checkpoints_ci/lunaris_codex_epoch-1_step-*.pt" 
          BEST_MODEL_FILE="./checkpoints_ci/best_model.pt"
          
          # Check if any file matches the main checkpoint pattern
          if ! ls $MAIN_CKPT_FILE_PATTERN 1> /dev/null 2>&1; then 
            echo "ERROR: Main checkpoint file matching '$MAIN_CKPT_FILE_PATTERN' not found!"
            exit 1
          fi
          if [ ! -f "$BEST_MODEL_FILE" ]; then 
            echo "ERROR: '$BEST_MODEL_FILE' not found!"
            exit 1
          fi
          echo "All expected checkpoint files found."

      - name: Test inference.py with toy model checkpoint
        if: success() && steps.check_ckpts.outcome == 'success'
        run: |
          CHECKPOINT_TO_TEST="./checkpoints_ci/best_model.pt"
          # Use --no_color for cleaner CI logs if inference.py uses rich
          OUTPUT=$(python inference.py \
            --checkpoint_path "$CHECKPOINT_TO_TEST" \
            --tokenizer_name_or_path gpt2 \
            --prompt "Test prompt:" \
            --max_new_tokens 5 \
            --temperature 0.5 \
            --device cpu --no_color) 
          
          if [ $? -eq 0 ] && [ -n "$OUTPUT" ]; then
            echo "Inference script ran successfully."
            echo "Generated output snippet (first 100 chars): ${OUTPUT:0:100}..."
          else
            echo "ERROR: Inference script failed or produced no output."
            echo "Output was: $OUTPUT" # Print output for debugging
            exit 1
          fi
      
      - name: Final Python CI Cleanup
        if: always()
        run: |
          echo "Cleaning up Python CI temporary files..."
          # rm -rf .venv-ci # Only if you create a specific venv within the CI steps
          rm -rf ./temp_data_ci_train ./temp_data_ci_val \
                 ./processed_data_ci ./checkpoints_ci \
                 ./coverage.xml
          echo "Python CI cleanup process complete."
