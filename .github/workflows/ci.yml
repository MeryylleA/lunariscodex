# .github/workflows/ci.yml
name: Lunaris Codex CI

on:
  push:
    branches: [ main ]
    paths: # For pushes to main, run if core files change
      - 'model.py'
      - 'prepare_data.py'
      - 'train.py'
      - 'inference.py'
      - 'text_cleaner/**'
      - 'data_analyzer/**'
      - 'bpe_trainer/**'
      - 'tests/**'
      - 'Makefile'
      - 'requirements.txt'
      - '.github/workflows/ci.yml'
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    branches: [ main ]
  workflow_dispatch:

env:
  # Global environment variables
  CXXFLAGS_MODE: RELEASE
  PYTHON_VERSION: '3.11'

jobs:
  # ==============================================================================
  # CHANGE DETECTION & SETUP
  # This job detects changes and sets up the matrix for subsequent jobs.
  # ==============================================================================
  detect_changes:
    name: Detect Changes & Setup Matrix
    runs-on: ubuntu-latest
    outputs:
      cpp_changes: ${{ steps.cpp_changes.outputs.any_changed }}
      python_changes: ${{ steps.python_changes.outputs.any_changed }}
      should_run_cpp: ${{ steps.decision.outputs.should_run_cpp }}
      should_run_python: ${{ steps.decision.outputs.should_run_python }}
      test_matrix: ${{ steps.matrix.outputs.matrix }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # Detect C++ related changes
      - name: Detect C++ Changes
        id: cpp_changes
        uses: tj-actions/changed-files@v46
        with:
          files: |
            Makefile
            text_cleaner/**
            data_analyzer/**
            bpe_trainer/**
            .github/workflows/ci.yml

      # Detect Python related changes
      - name: Detect Python Changes
        id: python_changes
        uses: tj-actions/changed-files@v46
        with:
          files: |
            model.py
            prepare_data.py
            train.py
            inference.py
            tests/**
            requirements.txt
            .github/workflows/ci.yml

      # Make execution decisions for each job type
      - name: Determine Job Execution
        id: decision
        run: |
          # C++ decision
          if [[ "${{ steps.cpp_changes.outputs.any_changed }}" == "true" || \
                "${{ github.event_name }}" == "push" || \
                "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "should_run_cpp=true" >> $GITHUB_OUTPUT
            echo "‚úÖ C++ jobs will run: Changes detected or triggered by push/manual dispatch"
          else
            echo "should_run_cpp=false" >> $GITHUB_OUTPUT
            echo "‚è≠Ô∏è C++ jobs will be skipped: No relevant changes detected"
          fi

          # Python decision
          if [[ "${{ steps.python_changes.outputs.any_changed }}" == "true" || \
                "${{ github.event_name }}" == "push" || \
                "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "should_run_python=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Python jobs will run: Changes detected or triggered by push/manual dispatch"
          else
            echo "should_run_python=false" >> $GITHUB_OUTPUT
            echo "‚è≠Ô∏è Python jobs will be skipped: No relevant changes detected"
          fi

      # Setup test matrix for cross-platform testing
      - name: Setup Test Matrix
        id: matrix
        run: |
          cat <<EOF > matrix.json
          {
            "os": [
              {
                "name": "ubuntu-latest",
                "display": "Ubuntu Latest",
                "runner": "ubuntu-latest",
                "make_cmd": "make",
                "cpp_ext": ""
              },
              {
                "name": "windows-latest",
                "display": "Windows Latest",
                "runner": "windows-latest",
                "make_cmd": "mingw32-make", # Use mingw32-make (if installed via MSYS2/choco) or MSBuild (more complex)
                "cpp_ext": ".exe"
              },
              {
                "name": "macos-latest",
                "display": "macOS Latest",
                "runner": "macos-latest",
                "make_cmd": "make",
                "cpp_ext": ""
              }
            ]
          }
          EOF

          echo "matrix=$(cat matrix.json | jq -c .)" >> $GITHUB_OUTPUT
          echo "üîß Test matrix configured for cross-platform testing"

  # ==============================================================================
  # C++ UTILITIES BUILD & TEST (Multi-OS)
  # This job builds and tests C++ utilities across different operating systems.
  # ==============================================================================
  cpp_utilities:
    name: C++ Build & Test (${{ matrix.os.display }})
    runs-on: ${{ matrix.os.runner }}
    needs: detect_changes
    # Only run if C++ changes detected OR if Python jobs should run (as Python might need C++ executables for future pybind11)
    if: needs.detect_changes.outputs.should_run_cpp == 'true' || needs.detect_changes.outputs.should_run_python == 'true'

    strategy:
      fail-fast: false # Continue testing other OS even if one fails
      matrix: ${{ fromJson(needs.detect_changes.outputs.test_matrix) }}

    outputs:
      text_cleaner_exec: ${{ steps.executables.outputs.text_cleaner_exec }}
      data_analyzer_exec: ${{ steps.executables.outputs.data_analyzer_exec }}
      bpe_processor_exec: ${{ steps.executables.outputs.bpe_processor_exec }}

    steps:
      # --- Repository Setup ---
      - name: Checkout Repository
        uses: actions/checkout@v4

      # --- OS-specific Setup ---
      - name: Install Build Dependencies (Linux)
        if: matrix.os.name == 'ubuntu-latest'
        run: |
          echo "üîß Installing C++ build dependencies on Ubuntu..."
          sudo apt-get update -y
          sudo apt-get install -y nlohmann-json3-dev build-essential
          echo "üìä System Information:"
          uname -a; gcc --version; g++ --version

      - name: Install Build Dependencies (Windows)
        if: matrix.os.name == 'windows-latest'
        run: |
          echo "üîß Installing C++ build dependencies on Windows..."
          # Install MSYS2 to get MinGW (GCC/G++) and mingw32-make
          choco install msys2 --params "/NoUpdate"
          echo "C:\tools\msys64\mingw64\bin" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
          echo "C:\tools\msys64\usr\bin" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
          
          # Use MSYS2 shell to install nlohmann-json if needed, or rely on CMake/vcpkg later
          # For Makefile, it might be easier if nlohmann-json is a header-only include or path is manually set.
          # For a simple Makefile, it often works if headers are just in the include path.
          
          # Ensure a proper 'make' utility is available (mingw32-make for MSYS2)
          echo "mingw32-make is now in PATH"
          # Display compiler info
          g++ --version || echo "g++ not found in PATH"
          make --version || echo "make not found in PATH" # Should be mingw32-make

      - name: Install Build Dependencies (macOS)
        if: matrix.os.name == 'macos-latest'
        run: |
          echo "üîß Installing C++ build dependencies on macOS..."
          # Clang and Make usually come with Xcode Command Line Tools.
          # For nlohmann-json, it might be via brew: brew install nlohmann-json
          # Or just rely on it being header-only or using find_package in CMake.
          echo "üìä System Information:"
          uname -a; clang --version; g++ --version || echo "g++ not found"

      # --- Build Cache Management ---
      - name: Clean Previous Build Artifacts
        run: |
          echo "üßπ Cleaning previous build artifacts..."
          # Use the specific 'make' command for the OS
          ${{ matrix.os.make_cmd }} clean || echo "‚ÑπÔ∏è No previous artifacts to clean or clean failed. Proceeding."

      - name: Cache C++ Executables
        id: cache_executables
        uses: actions/cache@v4
        with:
          path: |
            text_cleaner/*${{ matrix.os.cpp_ext }}
            data_analyzer/*${{ matrix.os.cpp_ext }}
            bpe_trainer/*${{ matrix.os.cpp_ext }}
          key: ${{ matrix.os.name }}-cpp-exec-${{ hashFiles('Makefile', 'text_cleaner/**/*.cpp', 'data_analyzer/**/*.cpp', 'bpe_trainer/**/*.cpp') }}
          restore-keys: |
            ${{ matrix.os.name }}-cpp-exec-

      # --- Build Process ---
      - name: Build C++ Utilities
        if: steps.cache_executables.outputs.cache-hit != 'true'
        run: |
          echo "üî® Building C++ utilities on ${{ matrix.os.display }}..."
          # Use the specific 'make' command for the OS
          ${{ matrix.os.make_cmd }} all CXXFLAGS_MODE=${{ env.CXXFLAGS_MODE }}
          echo "‚úÖ C++ utilities built successfully"

      - name: Set Executable Paths
        id: executables
        run: |
          # Paths should include the .exe extension for Windows
          echo "text_cleaner_exec=text_cleaner/lunaris_text_cleaner${{ matrix.os.cpp_ext }}" >> $GITHUB_OUTPUT
          echo "data_analyzer_exec=data_analyzer/lunaris_data_analyzer${{ matrix.os.cpp_ext }}" >> $GITHUB_OUTPUT
          echo "bpe_processor_exec=bpe_trainer/bpe_processor${{ matrix.os.cpp_ext }}" >> $GITHUB_OUTPUT

          if [[ "${{ steps.cache_executables.outputs.cache-hit }}" == "true" ]]; then
            echo "‚ôªÔ∏è C++ executables restored from cache"
          else
            echo "üÜï C++ executables built from source"
          fi

      # --- Functional Tests for C++ Utilities ---
      # These tests should be robust enough to run on different OS using bash shell
      - name: Run Text Cleaner Tests
        shell: bash # Ensure bash is used even on Windows runners
        run: |
          echo "üß™ Testing Text Cleaner utility on ${{ matrix.os.display }}..."
          
          # Setup test environment
          mkdir -p ./temp_text_cleaner_input ./temp_text_cleaner_output
          
          cat <<EOF > ./temp_text_cleaner_input/sample.txt
          <!DOCTYPE html>
          <html> <head><title>Test</title></head> <body>
          <!-- This is a comment -->
          <p>Hello   World!  </p>
          <script>alert("script content");</script>
          Another line.
          URL: http://example.com and email: test@example.com
          Duplicate Line
          Duplicate Line
          </body> </html>
          EOF

          # Run text cleaner
          TEXT_CLEANER_EXEC="./${{ steps.executables.outputs.text_cleaner_exec }}"
          # IMPORTANT: For Windows, paths in shell scripts should still use forward slashes for cross-platform compatibility
          # However, the executable itself might interpret them as backslashes. Let's make sure the path is correct.
          # The '$TEXT_CLEANER_EXEC' will correctly point to '.exe' on Windows.
          
          if [ ! -f "$TEXT_CLEANER_EXEC" ]; then
            echo "‚ùå ERROR: Text Cleaner executable not found at: $TEXT_CLEANER_EXEC"
            exit 1
          fi
          
          "$TEXT_CLEANER_EXEC" \
            --input ./temp_text_cleaner_input/sample.txt \
            --output ./temp_text_cleaner_output/cleaned.txt \
            --remove-html --normalize-whitespace --remove-empty-lines \
            --to-lowercase \
            --process-urls --url-placeholder "[url]" \
            --process-emails --email-placeholder "[email]" \
            --remove-exact-duplicates || {
              echo "‚ùå Text Cleaner execution failed"
              exit 1
            }
          
          # Create expected output
          cat <<-EXPECTED_EOF > expected_output.txt
          test
          hello world!
          another line with multiple spaces.
          email: [email]
          url: [url]
          duplicate line.
          final paragraph
EXPECTED_EOF
          
          # Compare outputs
          echo "üìä Comparing actual vs expected output..."
          # On Windows, 'diff' might not be available by default in git bash. Use git diff or powershell compare-object
          # Or, better, just rely on diff if bash shell is explicitly set.
          if diff -u expected_output.txt ./temp_text_cleaner_output/cleaned.txt; then
            echo "‚úÖ Text Cleaner test passed on ${{ matrix.os.display }}"
          else
            echo "‚ùå Text Cleaner test failed on ${{ matrix.os.display }}"
            exit 1
          fi

      - name: Run BPE Processor Tests
        shell: bash
        run: |
          echo "üß™ Testing BPE Processor on ${{ matrix.os.display }}..."
          
          # Setup test environment
          mkdir -p ./temp_bpe/corpus ./temp_bpe/model_output
          
          cat <<EOF > ./temp_bpe/corpus/corpus.txt
          hello world this is a test a test
          another line for another test of the bpe
          hello world again
          EOF

          # Train BPE model
          BPE_EXEC="./${{ steps.executables.outputs.bpe_processor_exec }}"
          if [ ! -f "$BPE_EXEC" ]; then
            echo "‚ùå ERROR: BPE Processor executable not found at: $BPE_EXEC"
            exit 1
          fi
          
          "$BPE_EXEC" \
            --action train \
            --corpus ./temp_bpe/corpus/corpus.txt \
            --vocab-size 270 \
            --output ./temp_bpe/model_output/bpe_model/ \
            --mode byte --verbose
            
          # Verify output files were created
          REQUIRED_FILES=(
            "./temp_bpe/model_output/bpe_model/bpe_model_lunaris.json"
            "./temp_bpe/model_output/bpe_model/merges_lunaris.txt"
            "./temp_bpe/model_output/bpe_model/vocabulary_lunaris.txt"
          )
          
          for file in "${REQUIRED_FILES[@]}"; do
            if [ ! -f "$file" ]; then
              echo "‚ùå ERROR: Required output file not found: $file"
              ls -R ./temp_bpe/model_output/
              exit 1
            fi
          done
          
          # Test tokenization
          TOKEN_OUTPUT=$("$BPE_EXEC" --action tokenize --model_path "./temp_bpe/model_output/bpe_model/" --input_text "hello test world" --verbose 2>/dev/null) # Redirect stderr to /dev/null
          
          if [ $? -ne 0 ] || [ -z "$TOKEN_OUTPUT" ]; then
            echo "‚ùå ERROR: BPE tokenization failed"
            exit 1
          fi
          
          # Basic validation: check for expected token IDs for "hello" (e.g., if 'hello' becomes one token)
          # This is simplified and might need refinement based on exact BPE output for this specific tiny corpus
          # For a more robust check, you might need to compare the full output string with an expected one
          echo "$TOKEN_OUTPUT" | grep -qE '\b[0-9]+\b' || { echo "‚ùå ERROR: Tokenization output is not numeric"; exit 1; }

          echo "‚úÖ BPE Processor test passed on ${{ matrix.os.display }}"
          echo "Token IDs (first 50 chars): ${TOKEN_OUTPUT:0:50}..."

      # --- Cleanup ---
      - name: Cleanup Test Files
        if: always()
        run: |
          echo "üßπ Cleaning up test files..."
          rm -rf ./temp_text_cleaner_input ./temp_text_cleaner_output \
                 ./expected_output.txt ./temp_bpe

  # ==============================================================================
  # PYTHON SUITE TESTS (Multi-OS)
  # This job runs Python-based tests across different operating systems.
  # ==============================================================================
  python_suite:
    name: Python Tests (${{ matrix.os.display }})
    runs-on: ${{ matrix.os.runner }}
    needs: detect_changes
    if: needs.detect_changes.outputs.should_run_python == 'true'

    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.detect_changes.outputs.test_matrix) }}

    steps:
      # --- Repository Setup ---
      - name: Checkout Repository
        uses: actions/checkout@v4

      # --- Python Environment Setup ---
      - name: Setup Python Environment
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Display Python Info
        run: |
          echo "üêç Python Environment on ${{ matrix.os.display }}:"
          python --version
          pip --version
          echo "Platform: $(python -c 'import platform; print(platform.platform())')"

      - name: Cache Python Dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ matrix.os.name }}-${{ env.PYTHON_VERSION }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ matrix.os.name }}-${{ env.PYTHON_VERSION }}-pip-

      - name: Install Python Dependencies
        run: |
          echo "üì¶ Installing Python dependencies on ${{ matrix.os.display }}..."
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          echo "‚úÖ Dependencies installed successfully"

      # --- Unit Tests ---
      - name: Run Unit Tests
        run: |
          echo "üß™ Running unit tests on ${{ matrix.os.display }}..."
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          pytest tests/ -k "test_model" \
            --cov=model \
            --cov-report=xml:coverage-${{ matrix.os.name }}.xml \
            --cov-report=term-missing \
            --tb=short
          echo "‚úÖ Unit tests completed on ${{ matrix.os.display }}"

      - name: Upload Coverage Report
        if: success()
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./coverage-${{ matrix.os.name }}.xml
          flags: model-tests-${{ matrix.os.name }}
          name: codecov-${{ matrix.os.name }}

      # --- Integration Tests Setup ---
      - name: Create Test Datasets
        run: |
          echo "üìù Creating test datasets on ${{ matrix.os.display }}..."

          # Training data
          mkdir -p ./temp_train_data
          echo "def train_function_one(): return 'train1'" > ./temp_train_data/train_sample_1.py
          echo "class TrainSampleClass:\n  value = 'train2'" > ./temp_train_data/train_sample_2.py

          # Validation data
          mkdir -p ./temp_val_data
          echo "def validation_function(): return 'validation_data_here'" > ./temp_val_data/val_sample_1.py
          echo "# Another validation line" > ./temp_val_data/val_sample_2.py

          echo "‚úÖ Test datasets created"

      - name: Test Data Preparation
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "üîÑ Testing data preparation on ${{ matrix.os.display }}..."

          # Prepare training data
          python prepare_data.py \
            --data_source_type text_file_lines \
            --dataset_name_or_path "./temp_train_data/*.py" \
            --tokenizer_name_or_path gpt2 \
            --output_path ./processed_data/train_data.memmap \
            --max_length 32 --max_examples 2 --overwrite_output \
            --tokenizer_trust_remote_code

          # Prepare validation data
          python prepare_data.py \
            --data_source_type text_file_lines \
            --dataset_name_or_path "./temp_val_data/*.py" \
            --tokenizer_name_or_path gpt2 \
            --output_path ./processed_data/val_data.memmap \
            --max_length 32 --max_examples 2 --overwrite_output \
            --tokenizer_trust_remote_code

          echo "‚úÖ Data preparation completed on ${{ matrix.os.display }}"

      - name: Test Training Pipeline
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "üèãÔ∏è Testing training pipeline on ${{ matrix.os.display }}..."
          python train.py \
            --memmap_file_train ./processed_data/train_data.memmap \
            --num_sequences_train 2 \
            --memmap_file_val ./processed_data/val_data.memmap \
            --num_sequences_val 2 \
            --tokenizer_name_or_path gpt2 \
            --dataset_max_length 32 --dataset_dtype int32 \
            --model_max_seq_len 32 --d_model 32 --n_layers 1 --n_heads 1 \
            --batch_size 1 --num_epochs 1 --device cpu \
            --checkpoint_dir ./checkpoints \
            --log_interval 1 --save_strategy epoch \
            --lora_rank 0 --seed 42
          echo "‚úÖ Training completed on ${{ matrix.os.display }}"

      - name: Verify Training Outputs
        id: verify_checkpoints
        run: |
          echo "üîç Verifying training checkpoint files on ${{ matrix.os.display }}..."
          echo "Checkpoint directory contents:"
          ls -R ./checkpoints
          
          # Check for expected files
          MAIN_CKPT_PATTERN="./checkpoints/lunaris_codex_epoch-1_step-*.pt" 
          BEST_MODEL_FILE="./checkpoints/best_model.pt"
          
          if ! ls $MAIN_CKPT_PATTERN 1> /dev/null 2>&1; then 
            echo "‚ùå ERROR: Main checkpoint file not found (pattern: $MAIN_CKPT_PATTERN)"
            exit 1
          fi
          
          if [ ! -f "$BEST_MODEL_FILE" ]; then 
            echo "‚ùå ERROR: Best model checkpoint not found: $BEST_MODEL_FILE"
            exit 1
          fi
          
          echo "‚úÖ All expected checkpoint files found on ${{ matrix.os.display }}"

      - name: Test Inference Pipeline
        if: success() && steps.verify_checkpoints.outcome == 'success'
        run: |
          echo "üîÆ Testing inference pipeline on ${{ matrix.os.display }}..."
          CHECKPOINT_PATH="./checkpoints/best_model.pt"
          
          OUTPUT=$(python inference.py \
            --checkpoint_path "$CHECKPOINT_PATH" \
            --tokenizer_name_or_path gpt2 \
            --prompt "Test prompt:" \
            --max_new_tokens 5 \
            --temperature 0.5 \
            --device cpu --no_color)
          
          if [ $? -eq 0 ] && [ -n "$OUTPUT" ]; then
            echo "‚úÖ Inference test passed on ${{ matrix.os.display }}"
            echo "Generated output (first 100 chars): ${OUTPUT:0:100}..."
          else
            echo "‚ùå ERROR: Inference script failed or produced no output on ${{ matrix.os.display }}"
            echo "Output: $OUTPUT" 
            exit 1
          fi

      # --- Cleanup ---
      - name: Cleanup Test Files
        if: always()
        run: |
          echo "üßπ Cleaning up test files on ${{ matrix.os.display }}..."
          rm -rf ./temp_train_data ./temp_val_data \
                 ./processed_data ./checkpoints \
                 ./coverage-${{ matrix.os.name }}.xml

  # ==============================================================================
  # FINAL STATUS REPORTING
  # This job aggregates results and posts comments on Pull Requests.
  # ==============================================================================
  ci_status:
    name: CI Status Report
    runs-on: ubuntu-latest
    needs: [detect_changes, cpp_utilities, python_suite]
    if: always()

    steps:
      - name: Determine Overall Status
        id: status
        run: |
          # Get results from matrix jobs. Default to 'skipped' if job was not run.
          # We need to collect results from all matrix elements.
          # Use a loop over the matrix to check.
          
          # Initialize overall success flag
          OVERALL_SUCCESS=true
          
          # Check if C++ jobs ran and succeeded
          if [[ "${{ needs.detect_changes.outputs.should_run_cpp }}" == "true" ]]; then
            # Access job status of matrix elements using JSON parsing
            # This is complex in shell, usually done by passing matrix results as outputs from cpp_utilities
            # For simplicity, we'll assume if the job ran, it's needs.cpp_utilities.result
            if [[ "${{ needs.cpp_utilities.result }}" != "success" ]]; then
              OVERALL_SUCCESS=false
              echo "‚ùå C++ utilities job failed or was cancelled."
            fi
          else
            echo "‚è≠Ô∏è C++ utilities job was skipped."
          fi
          
          # Check if Python jobs ran and succeeded
          if [[ "${{ needs.detect_changes.outputs.should_run_python }}" == "true" ]]; then
            if [[ "${{ needs.python_suite.result }}" != "success" ]]; then
              OVERALL_SUCCESS=false
              echo "‚ùå Python suite job failed or was cancelled."
            fi
          else
            echo "‚è≠Ô∏è Python suite job was skipped."
          fi
          
          echo "overall_success=$OVERALL_SUCCESS" >> $GITHUB_OUTPUT
          
          if [[ "$OVERALL_SUCCESS" == "true" ]]; then
            echo "‚úÖ Overall CI Status: SUCCESS"
          else
            echo "‚ùå Overall CI Status: FAILURE"
          fi

      - name: Post Success Comment (PR)
        if: github.event_name == 'pull_request' && steps.status.outputs.overall_success == 'true'
        uses: actions/github-script@v6 # Using github-script for more robust PR comments
        with:
          script: |
            const commentBody = `---
            **Lunaris Codex CI Status: SUCCESS ‚úÖ**
            Workflow: \`${{ github.workflow }}\`
            Branch: \`${{ github.head_ref }}\` (Commit: \`${{ github.sha }}\`)

            All tests passed successfully across multiple environments!
            üîó [View Action Details](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

            *This is an automated message from your Lunaris Codex CI system.*
            `;
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: commentBody
            });

      - name: Post Failure Comment (PR)
        if: github.event_name == 'pull_request' && steps.status.outputs.overall_success == 'false'
        uses: actions/github-script@v6 # Using github-script for more robust PR comments
        with:
          script: |
            const commentBody = `---
            **Lunaris Codex CI Status: FAILED ‚ùå**
            Workflow: \`${{ github.workflow }}\`
            Branch: \`${{ github.head_ref }}\` (Commit: \`${{ github.sha }}\`)

            Some tests failed. Please review the logs and fix any issues.
            üîó [View Action Details](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

            *This is an automated message from your Lunaris Codex CI system.*
            `;
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: commentBody
            });

      - name: Final Status Check
        run: |
          if [[ "${{ steps.status.outputs.overall_success }}" == "false" ]]; then
            echo "‚ùå CI Pipeline failed - exiting with error code"
            exit 1
          else
            echo "‚úÖ CI Pipeline completed successfully"
          fi
