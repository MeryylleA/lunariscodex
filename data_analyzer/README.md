# Lunaris Data Analyzer (`lunaris_data_analyzer`)

**Version: 0.3.0** (Updated for token decoding and JSON output)

`lunaris_data_analyzer` is a C++ command-line utility designed to inspect, validate, and analyze tokenized dataset files in the `.memmap` format used by the Lunaris Codex project (typically generated by a script like `prepare_data.py`).

It provides a fast way to verify dataset integrity, understand token distributions, view raw token sequences, and decode tokens to strings using a provided vocabulary. It leverages memory-mapping (`mmap`) on Linux for efficient file handling.

## Features

-   **File Validation:** Verifies that the `.memmap` file size matches the expected dimensions (number of sequences, max sequence length, and data type).
-   **Memory-Mapped Reading:** Uses `mmap` on Linux for efficient access to large dataset files. Includes an `ifstream` fallback for basic sequence printing on other systems or for testing (full statistics are best with `mmap`).
-   **Data Statistics:**
    -   Total token count.
    -   Count and percentage of a user-specified padding token ID.
    -   Count of out-of-vocabulary tokens (if `--vocab_size` is provided).
    -   Frequency report for the top N most common tokens.
-   **Sequence Inspection:** Prints the first N token ID sequences from the dataset.
-   **Token Decoding:** Decodes token IDs to human-readable strings in the output if a vocabulary file is provided.
-   **Flexible Output:** Supports both human-readable text output (default) and structured JSON output for statistics.
-   **Configurable:** All operations are controlled via command-line arguments.

## Prerequisites

-   A C++17 compatible compiler (e.g., `g++`, `clang++`).
-   Standard C++ libraries.
-   The `nlohmann/json.hpp` header for JSON parsing (vocabulary loading and JSON output). This should be available in the project's include path (e.g., via a `libs/` directory or system install).
-   For `mmap` functionality: A POSIX-compliant system (primarily tested on Linux).
-   `std::filesystem` support (C++17).

## Compilation

Navigate to the `data_analyzer/` directory containing `lunaris_data_analyzer.cpp` and compile using a C++17 compiler. Ensure `nlohmann/json.hpp` is in your include path.

A typical compilation command might be:
```bash
g++ lunaris_data_analyzer.cpp -o lunaris_data_analyzer -std=c++17 -O2 -I../libs
```
*(Assuming `nlohmann/json.hpp` is in a `libs` directory relative to the main project root, one level above `data_analyzer`)*

Adjust the `-I` path as necessary for `nlohmann/json.hpp`.
The `-O2` flag enables optimizations, which is recommended for performance.

On some older Linux systems or specific toolchains, you *might* need to link the filesystem library explicitly if you encounter linker errors related to `std::filesystem`:
```bash
g++ lunaris_data_analyzer.cpp -o lunaris_data_analyzer -std=c++17 -O2 -I../libs -lstdc++fs
```

## Usage

Run the compiled executable from your terminal, providing the necessary arguments:

```bash
./lunaris_data_analyzer --file <path_to_memmap> \
                        --num_sequences <count> \
                        --max_length <length> \
                        [--dtype <int32|int16>] \
                        [--pad_id <id_value>] \
                        [--vocab_size <size>] \
                        [--print_seq <num>] \
                        [--top_n_tokens <num>] \
                        [--vocab_file <path_to_vocab>] \
                        [--tokenizer_type <bpe|hf_json>] \
                        [--output_format <text|json>] \
                        [--no_mmap] \
                        [--help]
```

### Command-Line Arguments:

*   `--file <path>`: **(Required)** Path to the `.memmap` file.
*   `--num_sequences <long>`: **(Required)** Expected number of sequences in the dataset.
*   `--max_length <int>`: **(Required)** Expected maximum length of each sequence.
*   `--dtype <str>`: (Optional) Data type of the tokens (`int32`, `int16`). Default: `int32`.
*   `--pad_id <long>`: (Optional) Token ID used for padding. Used for padding statistics. Default: `0`.
*   `--vocab_size <int>`: (Optional) Expected vocabulary size for out-of-vocabulary token counting. Default: `-1` (disabled).
*   `--print_seq <num>`: (Optional) Number of initial sequences to print. Default: `3`.
*   `--top_n_tokens <num>`: (Optional) Report frequency of the top N most common tokens. Default: `0` (disabled). Only effective when `mmap` is used.
*   `--vocab_file <path>`: (Optional) Path to the vocabulary file for token decoding. If provided, `--tokenizer_type` is required.
*   `--tokenizer_type <type>`: (Optional) Type of tokenizer vocabulary. Required if `--vocab_file` is used.
    *   Supported types:
        *   `bpe`: Plain text file, one token string per line. Line number (0-indexed) is the token ID.
        *   `hf_json`: Hugging Face `tokenizer.json` file. Uses `model.vocab` field.
*   `--output_format <format>`: (Optional) Output format for statistics.
    *   Supported formats: `text` (default), `json`.
    *   If `json` is chosen, sequence printing is suppressed.
*   `--no_mmap`: (Optional) Disable `mmap` and use `ifstream`. Slower, and detailed statistics are not computed.
*   `-h`, `--help`: Display this help message and exit.

## Token Decoding

If you provide a vocabulary file, the tool will attempt to decode token IDs into their string representations in the output (for displayed sequences and top N tokens).

**Vocabulary Formats:**

*   **`bpe` (`--tokenizer_type bpe`)**:
    *   A plain text file where each line contains one token string.
    *   The token ID is implicitly its 0-indexed line number.
    *   Example:
        ```
        <unk>
        <s>
        </s>
        Ġhello
        Ġworld
        ```
        Here, `<unk>` is ID 0, `<s>` is ID 1, etc.
*   **`hf_json` (`--tokenizer_type hf_json`)**:
    *   A Hugging Face `tokenizer.json` file.
    *   The tool specifically looks for the `model.vocab` field, which should be a JSON object mapping token strings to their integer IDs.
    *   Example snippet from `tokenizer.json`:
        ```json
        {
          // ... other fields
          "model": {
            "type": "BPE",
            "vocab": {
              "<unk>": 0,
              "<s>": 1,
              "</s>": 2,
              "Ġhello": 29 hello, // Note: Actual ID is the value (e.g., 29414)
              "Ġworld": 1011 // Actual ID is the value (e.g., 1011)
              // ... many more tokens
            }
            // ... other model fields
          }
          // ... other fields
        }
        ```

**Example Command for Token Decoding:**

```bash
./lunaris_data_analyzer \
    --file ./processed_data/my_dataset.memmap \
    --num_sequences 10000 \
    --max_length 1024 \
    --vocab_file ./path/to/my_tokenizer.json \
    --tokenizer_type hf_json \
    --print_seq 2 \
    --top_n_tokens 5
```
This will print the first 2 sequences and top 5 tokens, with token IDs decoded using `my_tokenizer.json`.

## JSON Output

To get the dataset statistics in a structured JSON format (e.g., for programmatic use), use the `--output_format json` argument.

**JSON Output Content:**

The JSON output includes:
-   `total_tokens_in_file`
-   `padding_token_id_used_for_stats`
-   `padding_token_count`
-   `padding_token_percentage`
-   `out_of_vocabulary_tokens` (if `--vocab_size` is used)
-   `vocabulary_size_for_oov` (if `--vocab_size` is used)
-   `top_n_common_tokens`: An array of objects, each detailing `token_id`, `token_string` (if decoded, else `UNK` or `null`), and `count`.

**Note:** When `--output_format json` is active, the regular text-based sequence printing is suppressed to keep the output clean for JSON parsing. If `mmap` is not used (e.g., due to `--no_mmap` or `mmap` failure), the JSON output will indicate that statistics are unavailable.

**Example Command for JSON Output:**

```bash
./lunaris_data_analyzer \
    --file ./processed_data/my_dataset.memmap \
    --num_sequences 10000 \
    --max_length 1024 \
    --dtype int32 \
    --pad_id 0 \
    --top_n_tokens 10 \
    --output_format json > stats_output.json
```
This command will save the statistics for `my_dataset.memmap` into `stats_output.json`.

## Notes

-   Full statistical analysis (padding count, top N tokens, OOV count) is most efficiently performed when memory-mapping (`mmap`) is used. The `--no_mmap` option primarily supports basic sequence printing from the start of the file and does not compute these detailed statistics.
-   If `mmap` fails on a system where it's expected to work, ensure the file path is correct and the file is accessible.
-   The token decoding feature enhances readability but depends on the accuracy and format of the provided vocabulary file.
```
